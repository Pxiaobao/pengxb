# 自然语言处理

        自然语言处理 (NLP, Natural Language Processing) 又称为计算语言学，是一门借助计算机技术研究人类语言的科学。虽然 NLP 只有六七十年的历史，但是这门学科发展迅速且取得了令人印象深刻的成果。

## 机器是否要理解语言？

        自然语言处理六七十年的发展历程，基本可以分为两个阶段。

### 第一阶段：不懂语法怎么理解语言

        上世纪 50 年代到 70 年代，人们对用计算机处理自然语言的认识都局限在人类学习语言的方式上，用了二十多年时间苦苦探寻让计算机理解语言的方法，最终却一无所获。

        当时学术界普遍认为，要让机器完成 NLP 任务，首先必须让机器理解语言。因此分析语句和获取语义成为首要任务，而这主要依靠**语言学家人工总结文法规则**。特别是 60 年代基于乔姆斯基形式语言的编译器得到了很大的发展，更加鼓舞了人们通过概括语法规则来解决 NLP 问题的决心。

        但是人类语言既复杂又灵活，仅靠手工编写的文法规则根本无法覆盖，规则之间还可能存在矛盾。毕竟与规范严谨的程序语言不同，自然语言是一种复杂的上下文有关文法，实际很难用计算机进行解析。

        因此这一阶段，可以说自然语言处理的研究进入了一个误区。

### 第二阶段：只要看的足够多，就能处理语言

        正如人类是通过空气动力学而不是通过模仿鸟类造出了飞机，事实上进行自然语言处理也未必要让机器完全理解语言。70 年代，随着统计语言学的提出，基于数学模型和统计的方法开始兴起，NLP 领域才在接下来的四十多年里取得了一系列突破。

        当时，基于统计方法的核心模型是“**通信系统加隐马尔可夫模型**”，其输入和输出都是一维的符号序列，而且保持原有的次序（例如语音识别、词性分析），但是面对输出为二维树形结构的句法分析以及次序会有很大变化的机器翻译等任务，这种方法就难以解决了。

        80 年代以来，随着硬件计算能力的不断提高，以及互联网发展产生的海量数据，越来越多的统计机器学习方法被应用到自然语言处理中。例如随着**基于有向图的统计模型**的发展，机器已经能够进行复杂的句法分析，2005 年 Google 基于统计方法的翻译系统更是全面超过了基于规则的 SysTran 系统。

**基于神经网络方法的崛起**

        2006 年，随着 [Hinton](https://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527) 表明深度信念网络可以通过逐层预训练策略有效地训练，基于**神经网络和反向传播算法**进行训练的深度学习方法开始兴起，越来越多之前由于缺乏数据、计算能力以及有效优化方法而被忽视的神经网络模型得到了复兴。

> 例如 1997 年就已提出的长短时记忆网络 ([LSTM](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735))，直到如今，许多 LSTM 的变体依然在序列建模任务中广泛应用，包括工业界的许多自然语言处理模型。

        随着越来越多 NLP 研究者将注意力转向深度学习方法，各种神经网络模型也被引入到自然语言处理任务中（例如循环网络 LSTM 和卷积网络 CNN）。

        今天，已经没有人会质疑基于统计的 NLP 方法，但基于规则的传统方法也依然有其存在的价值。近年来许多研究都尝试将传统语言学特征融入到神经网络模型中，将语言学的研究成果与深度学习方法相结合。

## 如何建模语言？

        正如上面所说，人类语言是一种上下文相关的信息表达方式，因此要让机器能够处理自然语言，首先就要为自然语言建立数学模型，这个模型被称为 **统计语言模型**。

        统计语言模型的核心思想就是**判断一个文字序列是否构成人类能理解并且有意义的句子**（也就是判断一个句子是不是人话），这个问题曾经困扰了学术界很多年。

        20 世纪 70 年代之前，研究者试图从文字序列是否合乎文法、含义是否正确的角度入手，最终伴随着难以穷尽以及越来越繁琐的规则，这个问题依然无法有效解决。直到 IBM 实验室的 Jelinek 为了研究语音识别问题换了一个思路，用一个非常简单的统计模型就解决了这个问题。

Jelinek 的想法非常简单, 一个文字序列 $w_1, w_2, \ldots, w_n$ 是否合理, 就看这个句子 $S$ 出现的概率 $P(S)$ 如何, 出现概率越大的句子越合理:

$P(S)=P\left(w_1, w_2, \ldots, w_n\right)=P\left(w_1\right) P\left(w_2 \mid w_1\right) P\left(w_3 \mid w_1, w_2\right) \ldots P\left(w_n \mid w_1, w_2, \ldots, w_{n-1}\right)$

        任意一个词语 $w_n$ 的出现概率都取决于它前面出现的所有词（理论上也可以引入后面的词语共同预测单词的出现概率）。但是, 随着文本长度的增加, 条件概率 $P\left(w_n \mid w_1, w_2, \ldots, w_{n-1}\right)$ 会变得越来难计算, 因而实际计算时会假设每个词语 $w_i$ 的出现概率仅与它前面的 $N-1$ 个词语有关，即：$P\left(w_i \mid w_1, w_2, \ldots, w_{i-1}\right)=P\left(w_i \mid w_{i-N+1}, w_{i-N+2}, \ldots, w_{i-1}\right)$

        这种假设被称为**马尔可夫假设**, 对应的语言模型被称为 $N$ 元 (N-Gram) 模型。特别地, $N=2$ 时, 任意词语 $w_i$ 的出现概率只与它前面的词语 $w_{i-1}$ 有关, 被称为二元 (Bigram) 模型; 显然, $N=1$ 时的一元模型实际上就是一个上下文无关模型。由于 $N$ 元模型的空间和时间复杂度都几乎是 $N$ 的指数函数，因此实际应用中最常见的是 $N=3$ 的三元模型。

        但是，即使是三元、四元甚至是更高阶的语言模型，依然无法覆盖所有的语言现象。在自然语言中，上下文之间的相关性可能跨度非常大, 比如从一个段落跨到另一个段落, 这是马尔可夫假设解决不了的。此时就需要使用 LSTM、Transformer 等模型来捕获词语之间的长程依赖性 (long distance dependency) 了。

2017 年，Google 在前人工作的基础上提出了 [Attention](https://papers.nips.cc/paper/7181-attention-is-all-you-need) 注意力模型，为大家提供了另一种编码思路，论文中提出的 Transformer 结构更是引导了后续 NLP 模型的发展。

   

# 什么是词向量

# 

### ELMO

2018 年 [ELMO](https://arxiv.org/pdf/1802.05365.pdf) 模型的提出，直接在词向量端给出了一种简洁优雅的解决方案。与 Word2Vec 训练好之后就固定了的静态词向量不同，ELMO 会自动地根据词语的上下文信息去动态调整词语的词向量，因此自然就解决了多义词问题。

具体地，ELMO 首先利用语言模型进行预训练，然后在实际使用时，从预训练网络中提取各层的词向量拼接起来作为新的词向量。

![](https://transformers.run/assets/img/nnlm-to-bert/elmo.png)

ELMO 采用双层双向 LSTM 作为网络结构，从两个方向编码词语的上下文信息来进行预测，相当于将编码层直接封装到了语言模型中。训练完成后不仅学习到了词语的词向量，还训练好了一个双层双向的 LSTM 网络结构。对于每个词语，可以从第一层 LSTM 中得到包含句法信息的词向量，从第二层 LSTM 中得到包含语义信息的词向量……最终通过加权求和就可以得到每一个词语最终的词向量。

但是 ELMO 也依然存在缺陷，首先它使用 LSTM 作为编码器，而不是特征提取能力更强的 Transformer，其次直接通过拼接来融合双向抽取特征的方法也不够优美。

随后将 ELMO 中的 LSTM 更换为 Transformer 的 [GPT](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) 模型也很快出现了。但是 GPT 又再次追随了 NNLM 的脚步，只通过词语的上文来进行预测，这在很大程度上限制了模型的应用场景。例如阅读理解这种任务，如果预训练时候不把词语的下文嵌入到词向量中，会白白丢掉很多信息。

### BERT

2018 年底随着 [BERT](https://arxiv.org/pdf/1810.04805.pdf) 的提出，这一阶段神经语言模型的发展终于出现了一位集大成者，它在 11 个 NLP 任务上都达到了最好性能。

BERT 在模型大框架上采用和 GPT 完全相同的两阶段模型，首先是语言模型预训练，然后使用微调模式解决下游任务。BERT 不仅像 GPT 模型一样采用 Transformer 作为编码器，而且在预训练阶段采用了类似 ELMO 的双向语言模型。

![](https://transformers.run/assets/img/nnlm-to-bert/bert.png)

因此 BERT 不仅编码能力强大，而且对各种下游任务，Bert 都可以简单地通过改造输入输出部分来完成。但是 BERT 的优点同样也是它的缺陷，由于 BERT 构建的是双向语言模型，因而无法直接用于文本生成任务。

# 
