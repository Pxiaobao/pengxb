# 词向量

### **词向量是什么？**

        如何在神经网络中表示词汇？**在神经网络语境下的所谓的词汇表征，其实就是词汇的数据化、数字化**，一个直观 的想法是给词汇表中的所有单词各分配一个数字id，实现该想法的词向量方法被称作**one-hot独热编码**，即用一个只包含0和1的向量表征词汇表中的所有单词。假设词汇表规模为$|V|$，那么每个向量的长度均为$|V|$。每个单词对应的词向量只有在自己id对应的位置值为1，其他位置均为0。不难看出，如果把所有词向量写在一起的话，那么将会得到一个大小为$|V|*|V|$的对角矩阵，对角线值为1。

![](https://pic4.zhimg.com/v2-9b284d23ac6fee89af2e5115e7b8526f_r.jpg)

        然而这样用独热编码表示的词向量存在两个问题，**一是参数爆炸**，词向量维度等于词汇规模$|V|$，而词汇规模通常能够达到百万级别，这个参数规模过于庞大；二是采用独热编码的词向量会构成一个对角矩阵，这样**任意两个词向量之间都构成正交关系**，即**不存在单词相似性的概念**。因此我们需要一种更有效的词语表征方法来避免以上两个问题，该方法应该将词向量的维度$|D|$控制在一个远小于词汇规模的相对较低的水平，通常$50<|D|<1000$。除此之外，该向量表征方法还应表现词语之间的语义相似性特征，并通过向量之间的余弦值表示：

$$
\cos \left(\mathbf{w}^{(i)}, \mathbf{w}^{(j)}\right)=\frac{\mathbf{w}^{(i) T} \mathbf{w}^{(j)}}{\left\|\mathbf{w}^{(i)}\right\|_2 \cdot\left\|\mathbf{w}^{(j)}\right\|_2}
$$

        我们希望可以通过大量的无标记语料来训练出满足上述条件，即词向量维度控制在50到1000之间、可以表征词汇之间语义相似性的词向量，这样通过训练获得的词语表征形式也被称作**词嵌入（word embedding）**。词嵌入训练的理论思想源自**分布语义学**的一个假说，即“一个单词的特征可以用它所处的语境来概括（Firth，1957）。”基于此，词嵌入的基本思想是，对于出现在相似语境的单词，用来表征它们的词向量也应该相似。

### **Word2vec词嵌入模型**

        将神经网络语言模型发扬光大的是 2013 年 Google 提出的 [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf)，是最经典的词嵌入模型之一，获得了2023年NeurIPS时间检验奖。这个模型提供的词向量在很长一段时间里都是 NLP 模型的标配，即使是后来出现的 Glove 模型也难掩它的光芒。

        Word2Vec 的模型结构和 NNLM 基本一致，只是训练方法有所不同，分为 CBOW (Continuous Bag-of-Words) 和 Skip-gram 两种。

        这部分介绍的是通过**负取样（negative sampling**方法训练得到的word2vec模型。该方法把词向量作为模型的参数，通过训练得到词向量。该模型使用上下文信息进行训练，也就是说，词嵌入的学习是通过寻找目标词附近的词：如果一组词它们总是出现在同样的词语周围，那么这组词就会有比较相似的词嵌入。为了确定一个目标词的语境词，即目标词周围的词，我们需要确定一个**窗口值（window size）**。窗口值决定语境词的范围，比如窗口值为2，那么在句子“the pink horse is eating grass”中，目标词“horse”的语境词就是左边的两个词“the”和“pink”以及右边的两个词“is”和“eating”。

        根据目标函数的不同，负取样方法又可以分为“**跳词（Skip-gram）**”模型和“**词袋（CBOW, Continous Bag of Words)**”模型两类。“跳词”模型是给定目标词，预测语境词；而“词袋”模型则恰好相反，是给定语境词，预测目标词。

![](https://transformers.run/assets/img/nnlm-to-bert/word2vec.png)

        我们以“跳词”模型为例来看是如何使用负取样的方法训练词向量的。首先，我们利用语料库中实际出现的**词对（word pair）**  建立 **正训练集**，比如刚才的例子中目标词在“horse”在窗口值为2时在语料库中的语境词是“the”“pink”“is”和“eating”，如果写成词对形式的话就是(horse, the), (horse, pink), (horse , is)和(horse, eating)。这些词对就是正训练集的一部分。除了通过语料库建立正训练集，我们还需要通过随机取样的方法建立**负训练集**，即针对目标词，我们随机抽取一部分单词形成词对，这些随机抽取的单词我们不认为它们是目标词的语境词，因此这些词对被放入负训练集中。通过随机抽取建立负训练集的过程即为负取样，负取样的数量通常是正样本数的1-20倍。

正训练集我们用${pos}(\mathcal{O}) $ 表示，负训练集用${neg}(\mathcal{O})$表示，W 和 V分别表示目标词和语境词的词向量。那么用于训练word2vec词嵌入的神经网络模型的损失函数为

$$
L=\Pi_{(i, j) \in \operatorname{pos}(\mathcal{O})} P\left(\operatorname{pos} \mid \mathbf{w}^{(i)}, \mathbf{v}^{(j)}\right) \Pi_{\left(i^{\prime}, j^{\prime}\right) \in \operatorname{neg}(\mathcal{O})} P\left(\operatorname{neg} \mid \mathbf{w}^{\left(i^{\prime}\right)}, \mathbf{v}^{\left(j^{\prime}\right)}\right)
$$

        对于两个词向量 $\mathbf{w}$ 和 $\mathbf{v}$, 如果它们的向量内积越大, 那么它们同时出现的可能性越大, 因此我们把 $\mathbf{w}$ 和 $\mathbf{v}$ 词对标签为正, 即来自于正训练集的概率定义为向量内积的sigmoid函数值: 

$$
P(p o s \mid \mathbf{w}, \mathbf{v})=\sigma\left(\mathbf{w}^T \mathbf{v}\right) 。
$$

        这是一个二分类问题, 如果一个词对标签的不是正, 那么一定为负, 因此 

$$
P(n e g \mid \mathbf{w}, \mathbf{v})=1-P(\operatorname{pos} \mid \mathbf{w}, \mathbf{v})=1-\sigma\left(\mathbf{w}^T \mathbf{v}\right) 
$$

        因此, 我们的训练目标就是最大化训练集的似然值: $\mathcal{L}(\theta)=\Pi_i P\left(y^{(i)} \mid x^{(i)} ; \theta\right)$, 其中 $\theta$ 代表参数, 即目标词和语境词的词嵌入。 $x^{(i)}$ 代表来自正训练集和负训练集的词对。 $y^{(i)}$ 代表标签, 如果词对来自正训练集, 则标签为 1 , 如果来自负训练集则为 0 。在实际训练时, 我们对负对数似然值优化, 实际的损失函数为： 

$$
N L L(\theta)=-\log \mathcal{L}(\theta)=-\Sigma_i P\left(y^{(i)} \mid x^{(i)} ; \theta\right)
$$

使用随机梯度下降 (SGD, Stochastic Gradient Descent) 的方法进行优化, 目标词 $\mathbf{w}$ 和语境词 $\mathbf{v}$ 的词向量优化规则分别为 $\mathbf{w}_{\text {updated }} \leftarrow \mathbf{w}-\eta\left(\sigma\left(\mathbf{w}^T \mathbf{v}\right)-y\right) \mathbf{v}$ 和 $\mathbf{v}_{\text {updated }} \leftarrow \mathbf{v}-\eta\left(\sigma\left(\mathbf{w}^T \mathbf{v}\right)-y\right) \mathbf{w}$, 其中 $\eta>0$ 为学习率 (learning rate)。

### word2vec词嵌入模型的Python实现

我们以布朗语料库为例，按照上述负取样的方法训练一个word2vec词嵌入模型。

首先从nltk库中调出brown语料库，以token list的形式呈现：

```python
import nltk
nltk.download("brown")
from nltk.corpus import brown
corpus = brown.words()
```

我们让词汇大小为10000，为语料库中出现频率最大的前10000个单词创建一个词典来存储单词和ID的对应关系：

```python
from collections import Counter

VOCAB_SIZE = 10000

counter = Counter(corpus)
vocab = counter.most_common(VOCAB_SIZE)
word2id = {w[0]:n for n,w in enumerate(vocab)}
```

接下来我们建立训练数据集，包含负取样产生的负数据集和从语料库中获得的正数据集。我们将窗口值设为2，负取样数量定为正样本的10倍：

```python
import random
from tqdm import tqdm

WINDOW_SIZE = 2
NEG_SAMPLES_FACTORS = 10

train_set = list()
tokens = list(corpus)

for i, f_mid_token in tqdm(enumerate(tokens)):
  if f_mid_token in vocab:
    context_words = tokens[i+1, i+1+WINDOW_SIZE]
    f_mid_token_id = word2id[f_mid_token]
    for context_word in context_words:
      if context_word in vocab:
        context_word_id = word2id[context_word]
        train_set.append((f_mid_token_id, context_word_id, True))
        for i in range(NEG_SAMPLES_FACTORS):
          false_context_id = random.randint(0, VOCAB_SIZE-1)
          if false_context_id != context_word_id:
            train_set.append((f_mid_token_id, false_context_id, False))

  b_mid_token = corpus[i-VOCAB_SIZE]
  if b_mid_token in vocab:
    b_mid_token_id = word2id[b_mid_token_id]
    context_words = tokens[i-VOCAB_SIZE-WINDOW_SIZE: i-VOCAB_SIZE]
    for context_word in context_words:
      if context_word in vocab:
        context_word_id = word2id[context_word]
        train_set.append((b_mid_token_id, context_word_id, True))
        for i in range(NEG_SAMPLES_FACTORS):
          false_context_id = random.randint(0, VOCAB_SIZE-1)
          if false_context_id != context_word_id:
            train_set.append((f_mid_token_id, false_context_id, False))    
```

初始化原始词向量，我们设置词向量维度为50。

```python
import numpy as np

NUM_DIM = 50
target_word_matrix = 0.1 * np.random.randn(VOCAB_SIZE, NUM_DIM)
context_word_matrix = 0.1 * np.random.randn(VOCAB_SIZE, NUM_DIM)
```

接下来我们定义一个函数，用来更新初始化的词向量矩阵。在此之前我们先定义一个用来计算两个向量点积的Sigmoid函数值。

```python
import math

def sigmoid(x):
  dot_prod = vec1.dot(vec2)
  return 1/(1+math.exp(-dot_prod))

def update(target_id, context_id, label, lr):
  context_vec = context_word_matrix[context_id]
  target_vec = target_word_matrix[target_id]
  prob = sigmoid(context_vec, target_vec)
  new_vec_ctxt = context_vec + lr * (label - prob) * target_vec
  new_vec_tgt = target_vec + lr * (label-prob) * context_vec
  # update the embedding matrix
  context_word_matrix[context_id] = new_vec_ctxt
  target_word_matrix[target_id] = new_vec_tgt

  log_likelihood = math.log(prob) if label else math.log(1-prob)
  return log_likelihood
```

利用上面定义的更新函数以及通过负取样获取的训练集，我们对词向量矩阵进行优化。这里让训练集迭代2次，学习率设为0.1。

```python
EPOCH = 2
LR = 0.1

for _ in range(EPOCH):
  random.shuffle(train_set)
  log_likelihood = 0
  for tgt_id, ctxt_id, label in tqdm(train_set):
    log_likelihood += update(tgt_id, ctxt_id, label, LR)
  print("Log Likelihood: %.4f" % (log_likelihood))
```

经过训练之后的**目标词的词向量矩阵**即为通过负取样方法获得的word2vec词向量。我们可以利用word2vec词向量寻找和给定单词最相似的n个单词。

```python
def most_similar_words(word, n):
  if word not in word2id:
    return []
  w_id = word2id[word]
  vec =target_word_matrix[w_id, :]
  m = target_word_matrix
  dot_m_v = m.dot(vec.T) # n-dim vector
  dot_m_m = np.sum(m * m, axis=1) # n-dim vector
  dot_v_v = vec.dot(vec.T) # float
  sims = dot_m_v / (math.sqrt(dot_v_v) * np.sqrt(dot_m_m))
  return [id2word[idx] for idx in (-sims).argsort()[:n]]  

print(most_similar_words('mother', 10))
```

### FastText：对word2vec词向量模型的改进

word2vec词向量模型有一个很大的问题，就是尽管word2vec词向量是在一个非常大的语料库上训练的，词汇规模也可以设置的很大，但是在使用word2vec词向量时，仍有可能碰到训练语料库上没有出现过的**未知词**（**unknown words**），或是不在词汇表内的词，即**表外词**（Out-Of-Vocabulary Words, **OOV Words**)。面对这类词，word2vec词向量无力处理，为了解决表外词的问题，Facebook提出了FastText词向量方法，在word2vec模型的基础上对无法处理表外词的问题进行了改善。

Word2vec模型负取样的对象是单词层面，对token进行正取样和负取样，而FastText模型的基本思路是把n元字符（character n-gram）作为取样对象。对于每个单词，除了计算其本身的词向量以外，还计算其n元字符的词向量，一般包含一个单词的3元字符到6元字符。比如单词*remuneration*，它的3元字符到6元字符的集合为：

$$
\text { ngrams }(\text { remuneration })=\left\{\begin{array}{c}
\$ r e, \text { rem }, \ldots, \$ \text { rem }, \text { remu }, \ldots, \$ \text { remu }, \ldots, \text { ration }, \\
\text { ation } \$\}
\end{array}\right.
$$

于是, 对于词汇表内的单词, 通过FastText方法新生成的词向量是该单词原本的词向量与所有n元字符向量的均值。表外词的向量则为其包含的n元字符向量的均值。

已知单词: $\mathbf{w}^{(i)}=\frac{1}{|\operatorname{ngram}(i)|+1}\left[\mathbf{u}^{(i)}+\Sigma_{n \in \operatorname{ngrams}(i)} \mathbf{u}^{(n)}\right]$ ；
未知单词： $\mathbf{w}^{(i)}=\frac{1}{|\operatorname{ngram}(i)|} \Sigma_{n \in \operatorname{ngrams}(i)} \mathbf{u}^{(n)}$ 。
在训练时, 我们用新定义的词向量取代原先的词向量, 在反向传播时, 损失函数的梯度分配到原始词向量以及相关n元字符向量。

### 词嵌入在自然语言处理中的应用

        词向量本身包含词汇的语义信息，可以直接用来计算**词汇之间的相似度**。除此之外，作为单词语义表征的手段，词向量本身还可以作为**特征**直接应用于机器学习算法中来解决一些自然语言处理的任务，比如文本分类。在词嵌入基础上还可以计算句子级别甚至文本级别的嵌入，用于句子和文本级别的自然语言处理问题。

        以上是词嵌入直接应用于自然语言处理任务的一些方面。在基于深度神经网络的自然语言处理方法中，预训练好的词向量还可以用来初始化神经网络的嵌入层，这本质上是一种知识迁移的过程。在实践中，我们既可以在训练过程中把预训练的词嵌入作为参数的一部分参与训练进行微调，也可以选择冻结词嵌入，两种方法各有利弊。

        可以说，词嵌入是基于深度学习自然语言处理研究的基础；同时，词嵌入的思想也渗透在与自然语言处理相关的很多领域比如知识图谱、信息抽取、机器翻译等。

        虽然 Word2Vec 取得了巨大的成功，但是有一片乌云一直笼罩在词向量的上空——多义词问题。一词多义正是语言灵活性和高效性的体现，但 Word2Vec 却对此束手无策，无论词语的上下文如何，Word2Vec 对于一个词语只能提供一个词向量，即多义词被编码到了完全相同的参数空间。

        而事实上，早在上世纪 90 年代初，Yarowsky 就给出了一个非常简单又有效的解决方案——运用词语之间的**互信息**。具体地，对于一个多义词，分别从大量文本中找出这个词在表示不同语义时，同时出现的互信息最大的一些词。当在判别词语语义时，只需要看看上下文中哪些词语出现的多就可以了，即通过上下文来判断这个词语当前表达的语义。

        因此，在后来的几年中，标准 NLP 流程都是将 Word2Vec 预训练好的词向量作为模型的输入，然后通过 LSTM、CNN 等模型来重新对句子中的词语进行编码，以便捕获到词语的上下文信息。
