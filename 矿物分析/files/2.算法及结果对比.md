支持向量机（SVM）：

    支持向量机（Support Vector Machine, SVM）是一种强大的机器学习方法，用于分类和回归任务。在回归任务中，支持向量机的变体被称为支持向量回归（Support Vector Regression, SVR）。

代码示例：

```python
import numpy as np
import pandas as pd
import time
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

plt.rcParams['font.sans-serif'] = ['SimHei', 'Songti SC', 'STFangsong']
plt.rcParams['axes.unicode_minus'] = False

xx = ['wob_min','wob_max','wob_mean','wob_std','wob_skew','wob_kuet','T_min','T_max','T_mean','T_std','T_skew','T_kuet']

# 加载数据集
X = res[xx]
## Y为要预测的数值
y = res["静态抗压强度σc/MPa"]

# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 数据预处理：对特征进行标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 创建SVR模型
# 注意：SVR的默认核函数是RBF（径向基函数），你可以通过kernel参数更改核函数
svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)

# 训练模型
svr_model.fit(X_train_scaled, y_train)

# 进行预测
y_pred = svr_model.predict(X_test_scaled)

# 评估模型
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
r2score = r2_score(y_test, y_pred)
bst_rmse = mean_squared_error(y_test, y_pred) ** 0.5
print("均方误差(MSE):", mse)
print('R方：', r2score)
print('rmse:', bst_rmse)
```

```context
均方误差(MSE): 783.1521620608822
R方： 0.6557053709771413
rmse: 27.984855941399488
```

随机森林(random forest)：

随机森林进行回归分析是机器学习中的一个常见任务，可以有效地处理各种类型的回归问题。

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error

# 加载数据集
X = res[xx]
## Y为要预测的数值
y = res["静态抗压强度σc/MPa"]

# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建随机森林回归模型
# n_estimators表示树的数量，max_depth表示树的最大深度
rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42)

# 训练模型
rf_regressor.fit(X_train, y_train)

# 使用模型进行预测
y_pred = rf_regressor.predict(X_test)

# 评估模型
mse = mean_squared_error(y_test, y_pred)
r2score = r2_score(y_test, y_pred)
bst_rmse = mean_squared_error(y_test, y_pred) ** 0.5
print("均方误差(MSE):", mse)
print('R方：', r2score)
print('rmse:', bst_rmse)
```

```context
均方误差(MSE): 43.34981460072534
R方： 0.9809422624884407
rmse: 6.5840576091590615
```

GBDT :

梯度提升决策树（Gradient Boosted Decision Trees，GBDT）是一种强大的回归和分类模型，它通过迭代地训练决策树来最小化损失函数。

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error

# 加载数据集
X = res[xx]
## Y为要预测的数值
y = res["静态抗压强度σc/MPa"]

# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建GBDT回归模型
# n_estimators表示要训练的树的数量，max_depth表示每棵树的最大深度
gbdt_regressor = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)

# 训练模型
gbdt_regressor.fit(X_train, y_train)

# 使用模型进行预测
y_pred = gbdt_regressor.predict(X_test)

# 评估模型
mse = mean_squared_error(y_test, y_pred)
r2score = r2_score(y_test, y_pred)
bst_rmse = mean_squared_error(y_test, y_pred) ** 0.5
print("均方误差(MSE):", mse)
print('R方：', r2score)
print('rmse:', bst_rmse)
```

```context
均方误差(MSE): 66.75472824964271
R方： 0.9706528366878904
rmse: 8.170356678238882
```

Adboost:

AdaBoost（Adaptive Boosting）是一种集成学习方法，通过将多个弱学习器组合成一个强学习器来提高模型的预测性能。

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostRegressor
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor

# 加载数据集
X = res[xx]
## Y为要预测的数值
y = res["静态抗压强度σc/MPa"]

# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建AdaBoost回归模型，使用决策树作为基学习器
# n_estimators表示要训练的弱学习器的数量
ada_regressor = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=4),
                                  n_estimators=100, random_state=42)

# 训练模型
ada_regressor.fit(X_train, y_train)

# 使用模型进行预测
y_pred = ada_regressor.predict(X_test)

# 评估模型
mse = mean_squared_error(y_test, y_pred)
r2score = r2_score(y_test, y_pred)
bst_rmse = mean_squared_error(y_test, y_pred) ** 0.5
print("均方误差(MSE):", mse)
print('R方：', r2score)
print('rmse:', bst_rmse)
```

```context
均方误差(MSE): 67.4506798966619
R方： 0.9703468777366978
rmse: 8.21283628819313
```

LightGBM :

LightGBM是一个高效的梯度提升框架，由微软提供支持，它设计用于分布式和高效计算，尤其适合处理大规模数据。与其他梯度提升库相比，LightGBM具有更快的训练速度和更高的效率，同时还支持分类、回归及排名任务。

```python
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error

# 加载数据集
X = res[xx]
## Y为要预测的数值
y = res["静态抗压强度σc/MPa"]

# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建LightGBM数据格式
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# 设置模型参数
params = {
    'boosting_type': 'gbdt',  # 传统的梯度提升决策树
    'objective': 'regression',  # 回归任务
    'metric': 'l2',  # 评价指标，l2表示使用L2损失（均方误差MSE）
    'num_leaves': 31,  # 叶子节点数
    'learning_rate': 0.05,  # 学习率
    'feature_fraction': 0.9,  # 建树的特征选择比例
    'bagging_fraction': 0.8,  # 建树的样本采样比例
    'bagging_freq': 5,  # k表示每k次迭代执行bagging
    'verbose': 0  # <0 显示致命的, =0 显示错误 (警告), >0 显示信息
}

# 训练模型
gbm = lgb.train(params, train_data, num_boost_round=100, valid_sets=[train_data, test_data], early_stopping_rounds=10)

# 使用最佳迭代次数进行预测
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)

# 评估模型
mse = mean_squared_error(y_test, y_pred)
r2score = r2_score(y_test, y_pred)
bst_rmse = mean_squared_error(y_test, y_pred) ** 0.5
print("均方误差(MSE):", mse)
print('R方：', r2score)
print('rmse:', bst_rmse)
```

```context
均方误差(MSE): 37.86915708497443
R方： 0.9833517060647966
rmse: 6.1537920898397624
```

XGBoost：

```python
# xgboost回归器
import xgboost as xgb
from xgboost import plot_importance,plot_tree
import numpy as np
import pandas as pd
import time
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
plt.rcParams['font.sans-serif'] = ['SimHei', 'Songti SC', 'STFangsong']
plt.rcParams['axes.unicode_minus'] = False


xx = ['wob_min','wob_max','wob_mean','wob_std','wob_skew','wob_kuet','T_min','T_max','T_mean','T_std','T_skew','T_kuet']

## X为所有变量
#X = res.drop(['station','date','snd','SND','SND_SWE'], axis = 1)
X = res[xx]
## Y为要预测的数值
y = res["静态抗压强度σc/MPa"]
feature_name =X.columns
#将数据分割训练数据与测试数据
#print(np.isnan(X).any())
from sklearn.model_selection import train_test_split
# 随机采样20%的数据构建测试样本，其余作为训练样本
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,test_size=0.2,random_state=1)

dtrain = xgb.DMatrix(Xtrain, ytrain,feature_names=feature_name)
dtest = xgb.DMatrix(Xtest,feature_names=feature_name)

params = {'booster':'gbtree',
          'objective':'reg:squarederror',
          'eta':0.1,
          'gamma':0,
          'alpha':0,
          'lambda':3,
          'max_depth':6,
          'subsample':0.7,
          'colsample_bytree':0.9,
          'min_child_weight':1,
          'learning_rate':0.1,
          'seed':1000,
          'nthread':1
          }
num_round = 500

start_time = time.time()
bst = xgb.train(params, dtrain, num_round)
end_time = time.time()
print('The training time = {}'.format(end_time - start_time))

bst_ypred = bst.predict(dtest)
r2score = r2_score(ytest, bst_ypred)
bst_rmse = mean_squared_error(ytest, bst_ypred) ** 0.5

print('The R2_score = {}'.format(r2score))
print('The rmse of prediction is:', bst_rmse)
#plot_importance(bst,importance_type="weight")
plt.figure()
plt.plot(range(60), bst_ypred[:60], c="g", label="ypred", linewidth=2)
plt.plot(range(60), ytest[:60], c="r", label="ytest", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("静态抗压强度σc/MPa")
plt.show()
plt.show()
```

```context
The training time = 1.079322099685669
The R2_score = 0.9881620836493739
The rmse of prediction is: 5.277385320014107
```

使用遗传算法优化的xgboost模型：

### 遗传算法优化

遗传算法是一种基于自然选择和遗传学原理的搜索启发式算法，可以用来优化决策树的结构和参数。本研究将遗传算法用于：

- 选择最佳的特征和特征分裂点。
- 优化决策树的深度和复杂性，防止过拟合。
- 选择最佳的集成模型参数，如随机森林中树的数量、GBDT中的学习率等。

通过遗传算法，可以定义一个适应度函数（比如，交叉验证下的模型准确率），然后不断迭代，通过选择（选择最佳模型）、交叉（组合最佳模型的参数）和变异（随机改变参数），来寻找最优解。

```python
import random
import numpy as np
from deap import base, creator, tools
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split

xx = ['wob_min','wob_max','wob_mean','wob_std','wob_skew','wob_kuet','T_min','T_max','T_mean','T_std','T_skew','T_kuet']
X = res[xx]
## Y为要预测的数值
y = res["静态抗压强度"]

# 初始化creator
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

# 假设您已经有了如下参数范围
param_bounds = {
    'max_depth': (3, 10),  # 对于需要整数的参数，我们将使用randint
    'learning_rate': (0.01, 0.3),
    'subsample': (0.05, 1.0),
    'colsample_bytree': (0.05, 1.0),
}

# 初始化toolbox
toolbox = base.Toolbox()

# 注册属性生成器
toolbox.register("attr_max_depth", random.randint, *param_bounds['max_depth'])  # 整数生成器
for key, bound in filter(lambda item: item[0] != 'max_depth', param_bounds.items()):
    min_value, max_value = bound
    toolbox.register(f"attr_{key}", random.uniform, min_value, max_value)

# 创建个体生成器
toolbox.register("individual", tools.initCycle, creator.Individual,
                 [(toolbox.attr_max_depth if key == 'max_depth' else getattr(toolbox, f"attr_{key}"))
                  for key in param_bounds.keys()],
                 n=1)

# 创建种群生成器
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# 注册适应度函数
def eval_fitness(individual):
    params = dict(zip(param_bounds.keys(), individual))

    params['max_depth'] = int(params['max_depth'])
    params['learning_rate'] = params['learning_rate'] if (0 < params['learning_rate'] < 1) else random.uniform(0.1,0.3)
    params['subsample'] = params['subsample'] if (0< params['subsample']< 1) else random.uniform(0,1)
    params['colsample_bytree'] = params['colsample_bytree'] if (0< params['colsample_bytree']< 1) else random.uniform(0,1)

    print(params)
    # 使用新的参数训练模型
    model = XGBRegressor(**params)

    # 训练集和测试集划分保持不变
    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)
    dtrain = xgb.DMatrix(Xtrain, ytrain, feature_names=feature_name)
    dtest = xgb.DMatrix(Xtest, feature_names=feature_name)

    model.fit(Xtrain, ytrain)
    predictions = model.predict(Xtest)

    # 使用R²分数作为适应度函数
    fitness = r2_score(ytest, predictions)

    return fitness,

toolbox.register("evaluate", eval_fitness)

# 注册选择、交叉和变异算子
toolbox.register("select", tools.selTournament, tournsize=3)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.1)

# 设置遗传算法参数
POPULATION_SIZE = 50
MAX_GENERATIONS = 50
CXPB = 0.7
MUTPB = 0.2

# 运行遗传算法
pop = toolbox.population(n=POPULATION_SIZE)

for g in range(MAX_GENERATIONS):
    offspring = toolbox.select(pop, len(pop))
    offspring = list(map(toolbox.clone, offspring))

    for child1, child2 in zip(offspring[::2], offspring[1::2]):
        if random.random() < CXPB:
            toolbox.mate(child1, child2)
            del child1.fitness.values
            del child2.fitness.values

    for mutant in offspring:
        if random.random() < MUTPB:
            toolbox.mutate(mutant)
            del mutant.fitness.values

    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit

    pop[:] = offspring

# 找到最优个体及其对应的参数
best_individual = tools.selBest(pop, 1)[0]
best_params = dict(zip(param_bounds.keys(), best_individual))

# 输出最佳参数和对应的R²分数
print(f"Best Parameters: {best_params}")
print(f"Best Fitness (R² Score): {best_individual.fitness.values[0]}")
```

```
Best Parameters: {'max_depth': 7, 'learning_rate': 0.1970728149783871, 'subsample': 0.6849912226732566, 'colsample_bytree': 0.9269737580495006}
Best Fitness (R² Score): 0.9914082005663197
```







根据相关性整合结果：
